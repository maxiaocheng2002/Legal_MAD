import os
import json
import re

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì • (run_c1_evaluation.pyê°€ src/experiments ì•ˆì— ìˆìœ¼ë¯€ë¡œ)
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
RESULTS_DIR = os.path.join(PROJECT_ROOT, 'results')
EVALUATION_FILENAME = "evaluation_results_b2.json"

# --- 1. ì •ë‹µ ì¶”ì¶œ í•¨ìˆ˜ ---
def extract_answer(model_response: str) -> str or None:
    """
    ëª¨ë¸ ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ ìµœì¢… ì •ë‹µ (A, B, C, D)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    # 1. "Final Answer: (X)" í˜•íƒœë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    match_final = re.search(r'Final\s+Answer:\s*\(?([A-D])\)?', model_response, re.IGNORECASE)
    if match_final:
        return match_final.group(1).upper()

    # 2. ì‘ë‹µ ëì—ì„œ (X) í˜•íƒœë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    match_end = re.search(r'\([A-D]\)\s*$', model_response.strip())
    if match_end:
        return match_end.group(0).strip('()').upper()
    
    # 3. ë§Œì•½ ì‘ë‹µì´ ë‹¨ìˆœí•œ A, B, C, D ë¬¸ì í•˜ë‚˜ë§Œ í¬í•¨í•œë‹¤ë©´ ê·¸ê²ƒì„ ë°˜í™˜
    if model_response.strip().upper() in ['A', 'B', 'C', 'D']:
         return model_response.strip().upper()

    return None

# --- 2. í‰ê°€ ì‹¤í–‰ í•¨ìˆ˜ ---
def run_evaluation(experiment_name: str) -> dict:
    """
    ë‹¨ì¼ ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ì— ëŒ€í•œ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    filename = os.path.join(RESULTS_DIR, f"results_b2_cot_{experiment_name.replace(' ', '_')}.json")
    
    if not os.path.exists(filename):
        print(f"ğŸš¨ Error: Result file not found: {filename}")
        return {"total": 0, "correct": 0, "accuracy": 0.0, "error": "File Not Found"}

    with open(filename, 'r', encoding='utf-8') as f:
        results = json.load(f)
        
    total_questions = len(results)
    correct_answers = 0
    
    for item in results:
        expected = item.get('expected_answer', 'X').strip().upper()
        
        # ëª¨ë¸ ì‘ë‹µì´ í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° (ì˜ˆ: API ì—ëŸ¬) ê±´ë„ˆëœë‹ˆë‹¤.
        model_response = item.get('model_response')
        if not isinstance(model_response, str):
            continue 
            
        predicted = extract_answer(model_response)
        
        # ì •ë‹µì´ ìœ íš¨í•˜ê³  ì˜ˆì¸¡ëœ ì •ë‹µì´ ì •ë‹µê³¼ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
        if expected in ['A', 'B', 'C', 'D'] and predicted == expected:
            correct_answers += 1
            
    accuracy = (correct_answers / total_questions) * 100 if total_questions > 0 else 0.0
    
    return {
        "experiment_name": experiment_name,
        "total_questions": total_questions,
        "correct_answers": correct_answers,
        "accuracy": f"{accuracy:.2f}%"
    }

# --- 3. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ ---
if __name__ == "__main__":
    
    experiments = ["IRAC_CoT", "Basic_CoT"]
    all_evaluation_results = {}
    
    print("\n" + "="*70)
    print("ğŸš€ C1 - Evaluation Process Started")
    print(f"Loading results from: {RESULTS_DIR}")
    print("="*70)
    
    for exp in experiments:
        print(f"  -> Evaluating {exp}...")
        results = run_evaluation(exp)
        all_evaluation_results[exp] = results
        
        print(f"     âœ… {exp} Accuracy: {results.get('accuracy', 'N/A')}")
        print(f"     [Correct: {results.get('correct_answers')}/{results.get('total_questions')}]")
        print("-" * 50)
        
    # ìµœì¢… ê²°ê³¼ë¥¼ ë³„ë„ì˜ JSON íŒŒì¼ë¡œ ì €ì¥
    output_filename = os.path.join(RESULTS_DIR, EVALUATION_FILENAME)
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(all_evaluation_results, f, ensure_ascii=False, indent=4)
        
    print("\n" + "="*70)
    print("ğŸ‰ Evaluation Complete!")
    print(f"Detailed evaluation results saved to: {output_filename}")
    print("="*70)

